#!/usr/bin/env python3
"""
URL processor standalone test
"""

import asyncio

# ã‚·ãƒ³ãƒ—ãƒ«ãªURLæŠ½å‡ºã¨Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ
import re
import sys

import aiohttp
from bs4 import BeautifulSoup


class SimpleURLExtractor:
    """ç°¡å˜ãªURLæŠ½å‡ºæ©Ÿèƒ½"""

    def __init__(self):
        self.headers = {"User-Agent": "Mozilla/5.0 (compatible; TestBot/1.0)"}

    def extract_urls_from_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰URLã‚’æŠ½å‡º"""
        url_pattern = r'https?://[^\s<>"\'{}\|\\^`\[\]]+[^\s<>"\'{}\|\\^`\[\].,;!?)]'
        urls = re.findall(url_pattern, text, re.IGNORECASE)
        return list(set(urls))

    def is_valid_url(self, url):
        """URLå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        from urllib.parse import urlparse

        try:
            parsed = urlparse(url)
            return all(
                [
                    parsed.scheme in ("http", "https"),
                    parsed.netloc,
                    not any(
                        domain in parsed.netloc.lower()
                        for domain in [
                            "localhost",
                            "127.0.0.1",
                            "192.168.",
                            "10.",
                            "172.",
                        ]
                    ),
                ]
            )
        except Exception:
            return False


async def test_url_extraction():
    """URLæŠ½å‡ºã®ãƒ†ã‚¹ãƒˆ"""
    print("=== Testing URL Extraction ===")

    try:
        extractor = SimpleURLExtractor()

        # ãƒ†ã‚¹ãƒˆç”¨ãƒ†ã‚­ã‚¹ãƒˆ
        test_texts = [
            """
            ä»Šæ—¥ã¯é¢ç™½ã„è¨˜äº‹ã‚’è¦‹ã¤ã‘ã¾ã—ãŸï¼
            https://www.example.com/article1
            https://github.com/user/repo
            å‚è€ƒã«ãªã‚Šãã†ã§ã™ã€‚
            """,
            """
            Check out these links:
            https://docs.python.org/3/
            http://www.google.com
            https://stackoverflow.com/questions/123
            """,
            """
            ãƒãƒ¼ãƒãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚URLã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚
            """,
        ]

        for i, text in enumerate(test_texts):
            print(f"\nTest {i + 1}:")
            print(f"Text: {text.strip()}")

            urls = extractor.extract_urls_from_text(text)
            print(f"Extracted URLs: {urls}")

            valid_urls = [url for url in urls if extractor.is_valid_url(url)]
            print(f"Valid URLs: {valid_urls}")

        print("\nâœ“ URL extraction test completed successfully!")
        return True

    except Exception as e:
        print(f"âœ— URL extraction test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_web_scraping():
    """ç°¡å˜ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Testing Web Scraping ===")

    try:
        test_url = "https://httpbin.org/html"  # ãƒ†ã‚¹ãƒˆç”¨ã®HTMLãƒšãƒ¼ã‚¸

        timeout = aiohttp.ClientTimeout(total=10)
        headers = {"User-Agent": "Mozilla/5.0 (compatible; TestBot/1.0)"}

        async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
            try:
                async with session.get(test_url) as response:
                    print(f"Status: {response.status}")

                    if response.status == 200:
                        content = await response.text()

                        # HTMLãƒ‘ãƒ¼ã‚¹
                        soup = BeautifulSoup(content, "html.parser")
                        title = soup.find("title")

                        print(f"Title: {title.get_text() if title else 'No title'}")
                        print(f"Content length: {len(content)} characters")

                        # ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                        for script in soup(["script", "style"]):
                            script.decompose()

                        text = soup.get_text()
                        lines = [
                            line.strip() for line in text.split("\n") if line.strip()
                        ]
                        clean_text = "\n".join(lines[:10])  # æœ€åˆã®10è¡Œ

                        print(f"Extracted text preview:\n{clean_text}")

                        print("âœ“ Web scraping test completed successfully!")
                        return True
                    else:
                        print(f"âœ— HTTP error: {response.status}")
                        return False

            except aiohttp.ClientError as e:
                print(f"âœ— Client error: {e}")
                return False

    except Exception as e:
        print(f"âœ— Web scraping test failed: {e}")
        return False


async def test_url_processing():
    """URLå‡¦ç†ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
    print("\n=== Testing URL Processing Integration ===")

    try:
        extractor = SimpleURLExtractor()

        # URLä»˜ããƒ†ã‚­ã‚¹ãƒˆ
        text_with_urls = """
        AIã«é–¢ã™ã‚‹èˆˆå‘³æ·±ã„è¨˜äº‹ã‚’è¦‹ã¤ã‘ã¾ã—ãŸ:
        https://httpbin.org/html
        
        ã“ã®è¨˜äº‹ã§ã¯ã€äººå·¥çŸ¥èƒ½ã®æœ€æ–°å‹•å‘ã«ã¤ã„ã¦è¿°ã¹ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
        å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚
        """

        print(f"Original text:\n{text_with_urls}")

        # URLæŠ½å‡º
        urls = extractor.extract_urls_from_text(text_with_urls)
        print(f"\nFound URLs: {urls}")

        # å„URLã®å‡¦ç†
        processed_urls = []
        for url in urls:
            if extractor.is_valid_url(url):
                print(f"\nProcessing: {url}")

                try:
                    timeout = aiohttp.ClientTimeout(total=10)
                    headers = {"User-Agent": "Mozilla/5.0 (compatible; TestBot/1.0)"}

                    async with aiohttp.ClientSession(
                        timeout=timeout, headers=headers
                    ) as session, session.get(url) as response:
                        if response.status == 200:
                            content = await response.text()
                            soup = BeautifulSoup(content, "html.parser")

                            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                            title = soup.find("title")
                            title_text = title.get_text() if title else "No title"

                            processed_urls.append(
                                {
                                    "url": url,
                                    "title": title_text,
                                    "status": "success",
                                    "content_length": len(content),
                                }
                            )

                            print(f"  âœ“ Title: {title_text}")
                            print(f"  âœ“ Content: {len(content)} chars")
                        else:
                            processed_urls.append(
                                {
                                    "url": url,
                                    "status": "error",
                                    "error": f"HTTP {response.status}",
                                }
                            )

                except Exception as e:
                    processed_urls.append(
                        {"url": url, "status": "error", "error": str(e)}
                    )
                    print(f"  âœ— Error: {e}")

        print("\nProcessed URLs summary:")
        for result in processed_urls:
            status = "âœ“" if result["status"] == "success" else "âœ—"
            print(f"  {status} {result['url']} - {result['status']}")

        print("âœ“ URL processing integration test completed!")
        return True

    except Exception as e:
        print(f"âœ— URL processing test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–¢æ•°"""
    print("Starting URL Processing Tests...")
    print("=" * 60)

    tests = [
        test_url_extraction,
        test_web_scraping,
        test_url_processing,
    ]

    results = []
    for test_func in tests:
        try:
            result = await test_func()
            results.append(result)
        except Exception as e:
            print(f"âœ— Test {test_func.__name__} failed with error: {e}")
            results.append(False)

    print("\n" + "=" * 60)
    passed = sum(results)
    total = len(results)

    if passed == total:
        print(f"âœ… All {total} URL processing tests passed!")
    else:
        print(f"âš ï¸  {passed}/{total} tests passed, {total - passed} failed")

    print("\nğŸ¯ URL processing functionality is working!")
    print("   - URL extraction from text")
    print("   - Web content fetching")
    print("   - HTML parsing and metadata extraction")

    return passed == total


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
