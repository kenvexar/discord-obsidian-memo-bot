"""
Advanced note analyzer with semantic search and AI-powered features
"""

import re
from datetime import datetime
from typing import Any, Union

from ..obsidian.file_manager import ObsidianFileManager
from ..utils.mixins import LoggerMixin
from .mock_processor import MockAIProcessor
from .processor import AIProcessor
from .url_processor import URLContentExtractor
from .vector_store import SemanticSearchResult, VectorStore

# Settings loaded lazily to avoid circular imports


class AdvancedNoteAnalyzer(LoggerMixin):
    """é«˜åº¦ãªãƒãƒ¼ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(
        self,
        obsidian_file_manager: "ObsidianFileManager",
        ai_processor: Union["AIProcessor", "MockAIProcessor"],
    ):
        """
        åˆæœŸåŒ–

        Args:
            obsidian_file_manager: Obsidianãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
            ai_processor: AIå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
        """
        self.file_manager = obsidian_file_manager
        self.ai_processor = ai_processor

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.vector_store = VectorStore(
            obsidian_file_manager=obsidian_file_manager, ai_processor=ai_processor
        )
        self.url_extractor = URLContentExtractor()

        # è¨­å®š
        self.max_related_notes = 10
        self.min_similarity_threshold = 0.3
        self.max_internal_links = 5

        self.logger.info("Advanced note analyzer initialized")

    async def analyze_note_content(
        self,
        content: str,
        title: str,
        file_path: str | None = None,
        include_url_processing: bool = True,
        include_related_notes: bool = True,
    ) -> dict[str, Any]:
        """
        ãƒãƒ¼ãƒˆå†…å®¹ã®åŒ…æ‹¬çš„ãªåˆ†æ

        Args:
            content: ãƒãƒ¼ãƒˆå†…å®¹
            title: ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒˆãƒ«
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            include_url_processing: URLå‡¦ç†ã‚’å«ã‚€ã‹ã©ã†ã‹
            include_related_notes: é–¢é€£ãƒãƒ¼ãƒˆåˆ†æã‚’å«ã‚€ã‹ã©ã†ã‹

        Returns:
            åˆ†æçµæœ
        """
        try:
            self.logger.info(
                "Starting comprehensive note analysis",
                title=title,
                content_length=len(content),
                file_path=file_path,
            )

            analysis_results = {}

            # 1. URLå†…å®¹å‡¦ç†ã¨è¦ç´„
            if include_url_processing:
                url_results = await self._process_urls_in_content(content)
                analysis_results["url_processing"] = url_results

                # URLè¦ç´„ã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«çµ±åˆ
                if url_results.get("summaries"):
                    content = await self._integrate_url_summaries(
                        content, url_results["summaries"]
                    )

            # 2. é–¢é€£ãƒãƒ¼ãƒˆåˆ†æ
            related_notes = []
            if include_related_notes:
                related_notes = await self._find_related_notes(
                    content, exclude_file=file_path
                )
                analysis_results["related_notes"] = {
                    "results": [
                        {
                            "file_path": note.file_path,
                            "title": note.title,
                            "similarity_score": note.similarity_score,
                            "content_preview": note.content_preview,
                        }
                        for note in related_notes
                    ]
                }

            # 3. å†…éƒ¨ãƒªãƒ³ã‚¯ææ¡ˆ
            internal_links = []
            if related_notes:
                internal_links = await self._generate_internal_links(
                    content, related_notes
                )
                analysis_results["internal_links"] = {"suggestions": internal_links}

            # 4. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ€çµ‚çµ±åˆ
            enhanced_content = await self._enhance_content_with_links(
                content, internal_links, analysis_results.get("url_processing", {})
            )
            analysis_results["enhanced_content"] = {"content": enhanced_content}

            # 5. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒãƒ¼ãƒˆã‚’è¿½åŠ ï¼ˆæ–°è¦ãƒãƒ¼ãƒˆã®å ´åˆï¼‰
            if file_path and enhanced_content:
                await self._add_to_vector_store(file_path, title, enhanced_content)

            analysis_results.update(
                {
                    "original_content": {"content": content},
                    "title": {"value": title},
                    "file_path": {"path": file_path or ""},
                    "analyzed_at": {"timestamp": datetime.now().isoformat()},
                    "content_stats": self._get_content_stats(content),
                }
            )

            self.logger.info(
                "Note analysis completed",
                title=title,
                related_notes_count=len(related_notes),
                internal_links_count=len(internal_links),
                urls_processed=len(
                    analysis_results.get("url_processing", {}).get("processed_urls", [])
                ),
            )

            return analysis_results

        except Exception as e:
            self.logger.error(
                "Failed to analyze note content",
                title=title,
                error=str(e),
                exc_info=True,
            )
            return {
                "error": str(e),
                "title": title,
                "file_path": file_path,
                "analyzed_at": datetime.now().isoformat(),
            }

    async def search_related_notes(
        self, query: str, limit: int = 10, min_similarity: float = 0.1
    ) -> list[dict[str, Any]]:
        """
        é–¢é€£ãƒãƒ¼ãƒˆã‚’æ¤œç´¢

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            limit: çµæœæ•°åˆ¶é™
            min_similarity: æœ€å°é¡ä¼¼åº¦

        Returns:
            æ¤œç´¢çµæœ
        """
        try:
            results = await self.vector_store.search_similar_notes(
                query_text=query, limit=limit, min_similarity=min_similarity
            )

            return [
                {
                    "file_path": result.file_path,
                    "title": result.title,
                    "similarity_score": result.similarity_score,
                    "content_preview": result.content_preview,
                    "metadata": result.metadata,
                }
                for result in results
            ]

        except Exception as e:
            self.logger.error("Failed to search related notes", error=str(e))
            return []

    async def rebuild_vector_index(self, force: bool = False) -> dict[str, Any]:
        """
        ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰

        Args:
            force: å¼·åˆ¶å†æ§‹ç¯‰

        Returns:
            å†æ§‹ç¯‰çµæœ
        """
        try:
            self.logger.info("Starting vector index rebuild", force=force)

            start_time = datetime.now()
            await self.vector_store.build_index(force_rebuild=force)
            end_time = datetime.now()

            duration = (end_time - start_time).total_seconds()
            stats = await self.vector_store.get_embedding_stats()

            result = {
                "success": True,
                "duration_seconds": duration,
                "stats": stats,
                "rebuilt_at": end_time.isoformat(),
            }

            self.logger.info(
                "Vector index rebuild completed",
                duration=duration,
                embeddings=stats.get("total_embeddings", 0),
            )

            return result

        except Exception as e:
            self.logger.error(
                "Failed to rebuild vector index", error=str(e), exc_info=True
            )
            return {
                "success": False,
                "error": str(e),
                "attempted_at": datetime.now().isoformat(),
            }

    async def _process_urls_in_content(self, content: str) -> dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®URLã‚’å‡¦ç†"""
        try:
            # URLã‚’æŠ½å‡ºãƒ»å‡¦ç†
            url_results = await self.url_extractor.process_urls_in_text(
                content, max_urls=3
            )

            if not url_results.get("processed_urls"):
                return url_results

            # å„URLã®å†…å®¹ã‚’è¦ç´„
            summaries = []
            for url_data in url_results["processed_urls"]:
                try:
                    summary = await self.ai_processor.summarize_url_content(
                        url_data["url"], url_data["content"]
                    )

                    if summary:
                        summaries.append(
                            {
                                "url": url_data["url"],
                                "title": url_data["title"],
                                "summary": summary,
                                "original_content_length": url_data["content_length"],
                            }
                        )

                except Exception as e:
                    self.logger.warning(
                        "Failed to summarize URL content",
                        url=url_data["url"],
                        error=str(e),
                    )

            url_results["summaries"] = summaries
            return url_results

        except Exception as e:
            self.logger.error("Failed to process URLs in content", error=str(e))
            return {"error": str(e)}

    async def _integrate_url_summaries(
        self, content: str, summaries: list[dict[str, Any]]
    ) -> str:
        """URLè¦ç´„ã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«çµ±åˆï¼ˆæœ‰åŠ¹ãªURLãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰"""
        try:
            if not summaries:
                return content

            # æ—¢å­˜ã®URLè¦ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é‡è¤‡ã‚’é¿ã‘ã‚‹
            if "## ğŸ“ URLè¦ç´„" in content:
                self.logger.debug("URLè¦ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return content

            # æœ‰åŠ¹ãªè¦ç´„ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            valid_summaries = []
            for summary_data in summaries:
                summary_text = summary_data.get("summary", "").strip()
                url = summary_data.get("url", "").strip()

                # æœ‰åŠ¹ãªURLã‹ãƒã‚§ãƒƒã‚¯ï¼ˆDiscordç„¡åŠ¹ãƒªãƒ³ã‚¯ãªã©ã‚’é™¤å¤–ï¼‰
                is_valid_url = (
                    url
                    and not url.endswith("/channels/")  # Discordç„¡åŠ¹ãƒªãƒ³ã‚¯
                    and "discord.com/channels/" not in url  # Discordä¸å®Œå…¨ãƒªãƒ³ã‚¯
                    and summary_text
                    and not summary_text.startswith(
                        "Discordã®ä¼šè©±ã®è¦ç´„ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“"
                    )
                    and "URLã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒãªã„" not in summary_text
                    and "ç®‡æ¡æ›¸ã3ç‚¹ã«ã‚ˆã‚‹è¦ç´„ã‚’ä½œæˆã§ãã¾ã›ã‚“" not in summary_text
                    and "URLã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¦è¦ç´„ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“"
                    not in summary_text
                    and "æä¾›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã¯" not in summary_text
                    and "ä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€æ­£ç¢ºãªè¦ç´„ã¯ã§ãã¾ã›ã‚“" not in summary_text
                )

                if is_valid_url:
                    valid_summaries.append(summary_data)

            # æœ‰åŠ¹ãªè¦ç´„ãŒãªã„å ´åˆã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ãªã„
            if not valid_summaries:
                self.logger.debug("æœ‰åŠ¹ãªURLè¦ç´„ãŒãªã„ãŸã‚ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã¾ã›ã‚“")
                return content

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ«å°¾ã«URLè¦ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            url_section_parts = ["\n\n## ğŸ“ URLè¦ç´„\n"]

            for summary_data in valid_summaries:
                url_section_parts.append(
                    f"### {summary_data['title']}\n"
                    f"ğŸ”— {summary_data['url']}\n\n"
                    f"{summary_data['summary']}\n"
                )

            return content + "".join(url_section_parts)

        except Exception as e:
            self.logger.warning("Failed to integrate URL summaries", error=str(e))
            return content

    async def _find_related_notes(
        self, content: str, exclude_file: str | None = None
    ) -> list[SemanticSearchResult]:
        """é–¢é€£ãƒãƒ¼ãƒˆã‚’æ¤œç´¢"""
        try:
            exclude_files = {exclude_file} if exclude_file else set()

            results = await self.vector_store.search_similar_notes(
                query_text=content,
                limit=self.max_related_notes,
                min_similarity=self.min_similarity_threshold,
                exclude_files=exclude_files,
            )

            return results

        except Exception as e:
            self.logger.error("Failed to find related notes", error=str(e))
            return []

    async def _generate_internal_links(
        self, content: str, related_notes: list[SemanticSearchResult]
    ) -> list[str]:
        """å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ"""
        try:
            if not related_notes:
                return []

            # SemanticSearchResultã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            related_notes_dict = [
                {
                    "title": note.title,
                    "similarity_score": note.similarity_score,
                    "content_preview": note.content_preview,
                    "file_path": note.file_path,
                }
                for note in related_notes[: self.max_internal_links]
            ]

            links = await self.ai_processor.generate_internal_links(
                content, related_notes_dict
            )

            return links

        except Exception as e:
            self.logger.error("Failed to generate internal links", error=str(e))
            return []

    async def _enhance_content_with_links(
        self,
        content: str,
        internal_links: list[str],
        url_processing_results: dict[str, Any],
    ) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ã—ã¦å¼·åŒ–"""
        try:
            enhanced_content = content

            # URLè¦ç´„ãŒæ—¢ã«è¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if url_processing_results.get("summaries"):
                url_summaries = url_processing_results["summaries"]
                enhanced_content = await self._integrate_url_summaries(
                    content, url_summaries
                )

            # å†…éƒ¨ãƒªãƒ³ã‚¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            if internal_links:
                links_section = "\n\n## ğŸ”— é–¢é€£ãƒãƒ¼ãƒˆ\n\n"
                links_section += "\n".join(internal_links)
                enhanced_content += links_section

            return enhanced_content

        except Exception as e:
            self.logger.warning("Failed to enhance content with links", error=str(e))
            return content

    async def _add_to_vector_store(
        self, file_path: str, title: str, content: str
    ) -> None:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒãƒ¼ãƒˆã‚’è¿½åŠ """
        try:
            await self.vector_store.add_note_embedding(
                file_path=file_path, title=title, content=content
            )

        except Exception as e:
            self.logger.warning(
                "Failed to add note to vector store", file_path=file_path, error=str(e)
            )

    def _get_content_stats(self, content: str) -> dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            lines = content.split("\n")
            words = content.split()

            # è¦‹å‡ºã—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            headers = re.findall(r"^#{1,6}\s+.+$", content, re.MULTILINE)

            # ãƒªãƒ³ã‚¯æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            markdown_links = re.findall(r"\[([^\]]+)\]\([^)]+\)", content)
            wiki_links = re.findall(r"\[\[([^\]]+)\]\]", content)

            return {
                "character_count": len(content),
                "word_count": len(words),
                "line_count": len(lines),
                "header_count": len(headers),
                "markdown_link_count": len(markdown_links),
                "wiki_link_count": len(wiki_links),
                "total_link_count": len(markdown_links) + len(wiki_links),
            }

        except Exception as e:
            self.logger.warning("Failed to get content stats", error=str(e))
            return {}

    async def get_system_stats(self) -> dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            vector_stats = await self.vector_store.get_embedding_stats()

            return {
                "vector_store": vector_stats,
                "analyzer_config": {
                    "max_related_notes": self.max_related_notes,
                    "min_similarity_threshold": self.min_similarity_threshold,
                    "max_internal_links": self.max_internal_links,
                },
                "system_status": "operational",
                "last_updated": datetime.now().isoformat(),
            }

        except Exception as e:
            self.logger.error("Failed to get system stats", error=str(e))
            return {"error": str(e)}
